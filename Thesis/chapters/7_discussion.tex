\chapter{Discussion} \label{discussion}

In the previous chapter, we presented the results of our experiments on a number of selected cities. We have shown that our ``15-Minute City'' algorithm is computationally efficient, as the running time on a single core is less than \textbf{10} seconds even for a city as large as London. We have also shown that the algorithm is flexible and can be used to measure the accessibility of different types of amenities, such as coffee shops, post offices, and supermarkets. Our algorithm can be used in many approaches to the ``15-Minute City'' concept, as many existing solutions relay on calculating an area reachable within 15 minutes (or $t$ minutes) from a given location as the first step. 

However, while conducting these experiments, we have encountered several challenges. The first challenge was the quality of the data. The data we used was obtained from OpenStreetMap, which is a crowdsourced map of the world. While OpenStreetMap is a valuable resource, it may not always accurate or complete. The map data we received was very missing, for example, there could be multiple edges between the same node, which resulted in a graph that could be simplified further. However, in our experiment, it was difficult to remove the ``correct'' nodes and edges without messing with the good quality data. The \verb|network_type| paramenter in the \verb|osmnx| library also did not solve this problem, as \verb|driving| network does not include pedestrian paths, while \verb|walk| network does not include small roads that may be shared with pedestrians without the presence of pedestrian paths. The technique we employed to insert service locations into the graph was also not fool-proof, as we relay on the coordinates given by the OpenStreetMap API, which may not always be accurate. Even if the coordinates are correct, there is no garentee that the entrances of these services are located facing the closest edge. 

The second challenge was the data processing time. The data processing time was very long, especially for large cities like London. The data processing time was also dependent on the number of amenities we wanted to measure the accessibility of, as we needed to insert a node that is the closest to the service, create two new edges between the node and the service, and remove the original edge. This process was repeated for each service, which resulted in a long processing time. For example, the processing time for London was over \textbf{1 hour} for \textbf{10000} supermarkets and \textbf{others}. Recalling the 3 challenges according to Lima and Costa, in order to bridge the gap between the planning practice and software development communities: ``data availability and quality'', ``computational cost'' and ``adaptability'' \cite{lima_quest_2023}. In this thesis, we have addressed the latter two challenges. However, the first challenge, ``data availability and quality'', clearly is still a significant challenge that needs to be addressed. An efficient algorithm to the ``15-Minute City'' would not be useful if there is a lack of a comprehensive framework for urban space planning and other users such as researchers from other fields outside of Computer Science.

The weights of the graph to our algorithm was purposedly set to be the time required to travel along the corresponding edge, in minutes. This promotes the freedom for the users to incorporate different characterise to the roads which could affect the travelling time, such as the the street width, slope, and density of (foot or not) traffic. For example, service locations in a building such as a shopping mall may seem close to each other on a 2D map, our input graph allows for the possibility to modify travelling time based on the level of the building the service is located. Locations where it may be unplesant to travel could also have artificial time added to the edge weights.

In terms of the proposed algorithm itself, there are a number of improvement that can be considered in the future. A notable improvement would be to allow the algorithm to calculate the $t$-Minute City for multiple $t$ at once. In our current implementation, we repeat the algorithm multiple times for each $t$ of interest. However, this is inefficient as the graph search algorithm always start from the same nodes and stop when we reach to $t$ minutes. A suggestion of the modification would be to assign a number to each reachable node, representing the least amount of time required to reach the node. In this setting, the algorithm only needs to be run once for multiple $t$-Minute City calculations.

Another improvement is to allow the algorithm to run in parallel. In our current implementation, the algorithm is run on a single core. However, the algorithm can be parallelised so that different service types can be searched within the graph by multiple threads. At the end of the parallelied process, each thread should return the label array $v.r$ for each node $v$ in the graph. The main thread can then combine the results from each thread to obtain the final result.